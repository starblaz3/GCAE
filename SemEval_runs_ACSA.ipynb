{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c913c04-a53f-414b-af43-60b9b4c2dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras.datasets import imdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import LSTM, Activation, Dropout, Dense, Input, Concatenate, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "import string\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten,MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb9d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File descriptors\n",
    "test_file = \"acsa_test.csv\"\n",
    "train_file = \"acsa_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683654e9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the bread is top notch as well.</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have to say they have one of the fastest del...</td>\n",
       "      <td>service</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food is always fresh and hot- ready to eat!</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>did i mention that the coffee is outstanding?</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>certainly not the best sushi in new york, howe...</td>\n",
       "      <td>ambience</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>i have never in my life sent back food before,...</td>\n",
       "      <td>food</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>i have never in my life sent back food before,...</td>\n",
       "      <td>service</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>although the restaurant itself is nice, i pref...</td>\n",
       "      <td>ambience</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>although the restaurant itself is nice, i pref...</td>\n",
       "      <td>food</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>879 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review    aspect sentiment\n",
       "0                      the bread is top notch as well.      food  positive\n",
       "1    i have to say they have one of the fastest del...   service  positive\n",
       "2          food is always fresh and hot- ready to eat!      food  positive\n",
       "3        did i mention that the coffee is outstanding?      food  positive\n",
       "4    certainly not the best sushi in new york, howe...  ambience  positive\n",
       "..                                                 ...       ...       ...\n",
       "874  i have never in my life sent back food before,...      food  negative\n",
       "875  i have never in my life sent back food before,...   service  negative\n",
       "876  although the restaurant itself is nice, i pref...  ambience  positive\n",
       "877  although the restaurant itself is nice, i pref...      food  negative\n",
       "878  creamy appetizers--taramasalata, eggplant sala...      food  positive\n",
       "\n",
       "[879 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre processing test data\n",
    "test_data = pd.read_csv(test_file)\n",
    "test_data['review'] = test_data['review'].astype(str)\n",
    "test_data['review'] = test_data['review'].str.lower()\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4a5e7fd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>but the waitstaff was so horrible to us.</td>\n",
       "      <td>service</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to be completely fair, the only redeeming fact...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to be completely fair, the only redeeming fact...</td>\n",
       "      <td>misc</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the food is uniformly exceptional, with a very...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>where gabriela personally greets you and recom...</td>\n",
       "      <td>service</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3013</th>\n",
       "      <td>i'm partial to the gnocchi.</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>note that they do not serve beer, you must bri...</td>\n",
       "      <td>service</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>better than the bagel shop on the corner, but ...</td>\n",
       "      <td>misc</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>but that is highly forgivable.</td>\n",
       "      <td>misc</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3017</th>\n",
       "      <td>from the appetizers we ate, the dim sum and ot...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3018 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review   aspect sentiment\n",
       "0              but the waitstaff was so horrible to us.  service  negative\n",
       "1     to be completely fair, the only redeeming fact...     food  positive\n",
       "2     to be completely fair, the only redeeming fact...     misc  negative\n",
       "3     the food is uniformly exceptional, with a very...     food  positive\n",
       "4     where gabriela personally greets you and recom...  service  positive\n",
       "...                                                 ...      ...       ...\n",
       "3013                        i'm partial to the gnocchi.     food  positive\n",
       "3014  note that they do not serve beer, you must bri...  service  negative\n",
       "3015  better than the bagel shop on the corner, but ...     misc  negative\n",
       "3016                     but that is highly forgivable.     misc  positive\n",
       "3017  from the appetizers we ate, the dim sum and ot...     food  positive\n",
       "\n",
       "[3018 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre processing train data\n",
    "train_data = pd.read_csv(train_file)\n",
    "train_data['review'] = train_data['review'].astype(str)\n",
    "train_data['review'] = train_data['review'].str.lower()\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc1adc6-05e5-4416-8656-0d51ddbc5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of stopwords\n",
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \n",
    "             \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n",
    "             \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \n",
    "             \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n",
    "             \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n",
    "             \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n",
    "             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\n",
    "             \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
    "             \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
    "             \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
    "             \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c244629-0124-4dbf-9d89-5d9a8f5f3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to process data using the stopwords list\n",
    "def remove_stopwords(data):\n",
    "    data['review without stopwords'] = data['review'].apply(lambda x : ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "    return data\n",
    "\n",
    "def remove_tags(string):\n",
    "    result = re.sub('<.*?>','',string)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53904607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-868cb5b0e9b9>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data_without_stopwords['clean_review'] = train_data_without_stopwords['clean_review'].str.replace('[{}]'.format(string.punctuation), ' ')\n"
     ]
    }
   ],
   "source": [
    "# Processing train data by removing stop words from reviews\n",
    "train_data_without_stopwords = remove_stopwords(train_data)\n",
    "train_data_without_stopwords['clean_review'] = train_data_without_stopwords['review without stopwords'].apply(lambda cw : remove_tags(cw))\n",
    "train_data_without_stopwords['clean_review'] = train_data_without_stopwords['clean_review'].str.replace('[{}]'.format(string.punctuation), ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af05d68-93e2-45a4-8c69-26a462fe99a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review without stopwords</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>but the waitstaff was so horrible to us.</td>\n",
       "      <td>service</td>\n",
       "      <td>negative</td>\n",
       "      <td>waitstaff horrible us.</td>\n",
       "      <td>waitstaff horrible us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to be completely fair, the only redeeming fact...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>completely fair, redeeming factor food, averag...</td>\n",
       "      <td>completely fair  redeeming factor food  averag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to be completely fair, the only redeeming fact...</td>\n",
       "      <td>misc</td>\n",
       "      <td>negative</td>\n",
       "      <td>completely fair, redeeming factor food, averag...</td>\n",
       "      <td>completely fair  redeeming factor food  averag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the food is uniformly exceptional, with a very...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>food uniformly exceptional, capable kitchen wi...</td>\n",
       "      <td>food uniformly exceptional  capable kitchen wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>where gabriela personally greets you and recom...</td>\n",
       "      <td>service</td>\n",
       "      <td>positive</td>\n",
       "      <td>gabriela personally greets recommends eat.</td>\n",
       "      <td>gabriela personally greets recommends eat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3013</th>\n",
       "      <td>i'm partial to the gnocchi.</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>partial gnocchi.</td>\n",
       "      <td>partial gnocchi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>note that they do not serve beer, you must bri...</td>\n",
       "      <td>service</td>\n",
       "      <td>negative</td>\n",
       "      <td>note not serve beer, must bring own.</td>\n",
       "      <td>note not serve beer  must bring own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>better than the bagel shop on the corner, but ...</td>\n",
       "      <td>misc</td>\n",
       "      <td>negative</td>\n",
       "      <td>better bagel shop corner, not worth going way ...</td>\n",
       "      <td>better bagel shop corner  not worth going way ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>but that is highly forgivable.</td>\n",
       "      <td>misc</td>\n",
       "      <td>positive</td>\n",
       "      <td>highly forgivable.</td>\n",
       "      <td>highly forgivable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3017</th>\n",
       "      <td>from the appetizers we ate, the dim sum and ot...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>appetizers ate, dim sum variety foods, impossi...</td>\n",
       "      <td>appetizers ate  dim sum variety foods  impossi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3018 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review   aspect sentiment  \\\n",
       "0              but the waitstaff was so horrible to us.  service  negative   \n",
       "1     to be completely fair, the only redeeming fact...     food  positive   \n",
       "2     to be completely fair, the only redeeming fact...     misc  negative   \n",
       "3     the food is uniformly exceptional, with a very...     food  positive   \n",
       "4     where gabriela personally greets you and recom...  service  positive   \n",
       "...                                                 ...      ...       ...   \n",
       "3013                        i'm partial to the gnocchi.     food  positive   \n",
       "3014  note that they do not serve beer, you must bri...  service  negative   \n",
       "3015  better than the bagel shop on the corner, but ...     misc  negative   \n",
       "3016                     but that is highly forgivable.     misc  positive   \n",
       "3017  from the appetizers we ate, the dim sum and ot...     food  positive   \n",
       "\n",
       "                               review without stopwords  \\\n",
       "0                                waitstaff horrible us.   \n",
       "1     completely fair, redeeming factor food, averag...   \n",
       "2     completely fair, redeeming factor food, averag...   \n",
       "3     food uniformly exceptional, capable kitchen wi...   \n",
       "4            gabriela personally greets recommends eat.   \n",
       "...                                                 ...   \n",
       "3013                                   partial gnocchi.   \n",
       "3014               note not serve beer, must bring own.   \n",
       "3015  better bagel shop corner, not worth going way ...   \n",
       "3016                                 highly forgivable.   \n",
       "3017  appetizers ate, dim sum variety foods, impossi...   \n",
       "\n",
       "                                           clean_review  \n",
       "0                                waitstaff horrible us   \n",
       "1     completely fair  redeeming factor food  averag...  \n",
       "2     completely fair  redeeming factor food  averag...  \n",
       "3     food uniformly exceptional  capable kitchen wi...  \n",
       "4            gabriela personally greets recommends eat   \n",
       "...                                                 ...  \n",
       "3013                                   partial gnocchi   \n",
       "3014               note not serve beer  must bring own   \n",
       "3015  better bagel shop corner  not worth going way ...  \n",
       "3016                                 highly forgivable   \n",
       "3017  appetizers ate  dim sum variety foods  impossi...  \n",
       "\n",
       "[3018 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12968419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-04e680a37cae>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_data_without_stopwords['clean_review'] = test_data_without_stopwords['clean_review'].str.replace('[{}]'.format(string.punctuation), ' ')\n"
     ]
    }
   ],
   "source": [
    "# Processing test data by removing stop words from reviews\n",
    "test_data_without_stopwords = remove_stopwords(test_data)\n",
    "test_data_without_stopwords['clean_review']= test_data_without_stopwords['review without stopwords'].apply(lambda cw : remove_tags(cw))\n",
    "test_data_without_stopwords['clean_review'] = test_data_without_stopwords['clean_review'].str.replace('[{}]'.format(string.punctuation), ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b4be42",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review without stopwords</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the bread is top notch as well.</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>bread top notch well.</td>\n",
       "      <td>bread top notch well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have to say they have one of the fastest del...</td>\n",
       "      <td>service</td>\n",
       "      <td>positive</td>\n",
       "      <td>say one fastest delivery times city.</td>\n",
       "      <td>say one fastest delivery times city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food is always fresh and hot- ready to eat!</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>food always fresh hot- ready eat!</td>\n",
       "      <td>food always fresh hot  ready eat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>did i mention that the coffee is outstanding?</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>mention coffee outstanding?</td>\n",
       "      <td>mention coffee outstanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>certainly not the best sushi in new york, howe...</td>\n",
       "      <td>ambience</td>\n",
       "      <td>positive</td>\n",
       "      <td>certainly not best sushi new york, however, al...</td>\n",
       "      <td>certainly not best sushi new york  however  al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>i have never in my life sent back food before,...</td>\n",
       "      <td>food</td>\n",
       "      <td>negative</td>\n",
       "      <td>never life sent back food before, simply to, w...</td>\n",
       "      <td>never life sent back food before  simply to  w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>i have never in my life sent back food before,...</td>\n",
       "      <td>service</td>\n",
       "      <td>negative</td>\n",
       "      <td>never life sent back food before, simply to, w...</td>\n",
       "      <td>never life sent back food before  simply to  w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>although the restaurant itself is nice, i pref...</td>\n",
       "      <td>ambience</td>\n",
       "      <td>positive</td>\n",
       "      <td>although restaurant nice, prefer not go food.</td>\n",
       "      <td>although restaurant nice  prefer not go food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>although the restaurant itself is nice, i pref...</td>\n",
       "      <td>food</td>\n",
       "      <td>negative</td>\n",
       "      <td>although restaurant nice, prefer not go food.</td>\n",
       "      <td>although restaurant nice  prefer not go food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>creamy appetizers  taramasalata  eggplant sala...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>879 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                review    aspect sentiment  \\\n",
       "0                      the bread is top notch as well.      food  positive   \n",
       "1    i have to say they have one of the fastest del...   service  positive   \n",
       "2          food is always fresh and hot- ready to eat!      food  positive   \n",
       "3        did i mention that the coffee is outstanding?      food  positive   \n",
       "4    certainly not the best sushi in new york, howe...  ambience  positive   \n",
       "..                                                 ...       ...       ...   \n",
       "874  i have never in my life sent back food before,...      food  negative   \n",
       "875  i have never in my life sent back food before,...   service  negative   \n",
       "876  although the restaurant itself is nice, i pref...  ambience  positive   \n",
       "877  although the restaurant itself is nice, i pref...      food  negative   \n",
       "878  creamy appetizers--taramasalata, eggplant sala...      food  positive   \n",
       "\n",
       "                              review without stopwords  \\\n",
       "0                                bread top notch well.   \n",
       "1                 say one fastest delivery times city.   \n",
       "2                    food always fresh hot- ready eat!   \n",
       "3                          mention coffee outstanding?   \n",
       "4    certainly not best sushi new york, however, al...   \n",
       "..                                                 ...   \n",
       "874  never life sent back food before, simply to, w...   \n",
       "875  never life sent back food before, simply to, w...   \n",
       "876      although restaurant nice, prefer not go food.   \n",
       "877      although restaurant nice, prefer not go food.   \n",
       "878  creamy appetizers--taramasalata, eggplant sala...   \n",
       "\n",
       "                                          clean_review  \n",
       "0                                bread top notch well   \n",
       "1                 say one fastest delivery times city   \n",
       "2                    food always fresh hot  ready eat   \n",
       "3                          mention coffee outstanding   \n",
       "4    certainly not best sushi new york  however  al...  \n",
       "..                                                 ...  \n",
       "874  never life sent back food before  simply to  w...  \n",
       "875  never life sent back food before  simply to  w...  \n",
       "876      although restaurant nice  prefer not go food   \n",
       "877      although restaurant nice  prefer not go food   \n",
       "878  creamy appetizers  taramasalata  eggplant sala...  \n",
       "\n",
       "[879 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "996868d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data into a list, seperating reviews from sentiment polarity\n",
    "train_reviews_list = []\n",
    "train_sentiment = []\n",
    "train_aspect = []\n",
    "for i in range(len(train_data_without_stopwords)):\n",
    "    train_reviews_list.append(train_data_without_stopwords.iloc[i,3])\n",
    "    train_sentiment.append(train_data_without_stopwords.loc[i,'sentiment'])\n",
    "    train_aspect.append(train_data_without_stopwords.loc[i,'aspect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a481913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data into a list, seperating reviews from sentiment polarity\n",
    "test_reviews_list = []\n",
    "test_sentiment = []\n",
    "test_aspect = []\n",
    "for i in range(len(test_data_without_stopwords)):\n",
    "    test_reviews_list.append(test_data_without_stopwords.iloc[i,3])\n",
    "    test_sentiment.append(test_data_without_stopwords.loc[i,'sentiment'])\n",
    "    test_aspect.append(test_data_without_stopwords.loc[i,'aspect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7739b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining train x and y values\n",
    "Y_train = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, train_sentiment)))\n",
    "X_train = train_reviews_list\n",
    "X_aspect_train = train_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e518b4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c62c4a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining train x and y values\n",
    "Y_test = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, test_sentiment)))\n",
    "X_test = test_reviews_list\n",
    "X_aspect_test = test_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f25c95b7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3a9cd6a-e045-4560-8e76-359010f7f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing data\n",
    "tokenizer1 = Tokenizer(num_words=5000)\n",
    "tokenizer1.fit_on_texts(X_train)\n",
    "words_to_index = tokenizer1.word_index\n",
    "tokenizer2 = Tokenizer(num_words=5000)\n",
    "tokenizer2.fit_on_texts(X_aspect_train)\n",
    "aspect_to_index = tokenizer2.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16998371-fa77-49c9-89f6-87fb3312e2c6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to read the GloVe vectors for embedding\n",
    "def read_glove_vector(glove_vec):\n",
    "    with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            words_in_the_line = line.split()\n",
    "            current_word = words_in_the_line[0]\n",
    "            word_to_vec_map[current_word] = np.array(words_in_the_line[1:], dtype=np.float64)\n",
    "    return word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "384e0e11-b901-4a92-843e-bcc249e88140",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Loading the GloVe vectors\n",
    "word_to_vec_map = read_glove_vector('./glove.6B.300d.txt')\n",
    "\n",
    "maxLen = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b8405b7-b36e-42bb-92fe-dd7ffca22921",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits :  3611  misses :  158\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "vocab_len = len(words_to_index)+1\n",
    "embed_vector_len = maxLen\n",
    "\n",
    "embed_matrix = np.zeros((vocab_len, embed_vector_len))\n",
    "hits = 0\n",
    "count = 0\n",
    "for word, index in words_to_index.items():\n",
    "    count +=1\n",
    "    embedding_vector = word_to_vec_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        hits += 1\n",
    "        embed_matrix[index, :] = embedding_vector\n",
    "print(\"hits : \", hits,\" misses : \",count-hits)\n",
    "embedding_layer = Embedding(input_dim=vocab_len,\n",
    "                            output_dim=embed_vector_len,\n",
    "                            input_length=maxLen, weights = [embed_matrix],\n",
    "                            trainable=False)\n",
    "\n",
    "vocab_asp_len = len(aspect_to_index)+1\n",
    "embed_vector_len = maxLen\n",
    "\n",
    "embed_matrix = np.zeros((vocab_asp_len, embed_vector_len))\n",
    "\n",
    "for word, index in aspect_to_index.items():\n",
    "    embedding_vector = word_to_vec_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embed_matrix[index, :] = embedding_vector\n",
    "\n",
    "aspect_embedding_layer = Embedding(input_dim=vocab_asp_len,\n",
    "                            output_dim=embed_vector_len,\n",
    "                            input_length=maxLen, weights = [embed_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9937af70-cb37-4d5d-9dc8-d254d6dd8e26",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x7fc02c585df0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56793c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x7fbfdf9f4b20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f7455e1-cbef-4461-8117-ab1bfbc971e3",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_indices = tokenizer1.texts_to_sequences(X_train)\n",
    "\n",
    "X_train_indices = pad_sequences(X_train_indices, maxlen=maxLen, padding='post')\n",
    "\n",
    "X_aspect_indices = tokenizer2.texts_to_sequences(X_aspect_train)\n",
    "\n",
    "X_aspect_indices = pad_sequences(X_aspect_indices, maxlen=maxLen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17340297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_LSTM_model(emb):\n",
    "    embedding_vecor_length = maxLen\n",
    "    model = Sequential()\n",
    "    model.add(emb)\n",
    "    model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dense(128, activation='sigmoid'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "243299f6-13a5-4213-9a50-0f8b908d5284",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Model definition for a LSTM based model\n",
    "def LSTM_model(input_shape):\n",
    "    X_indices = Input(input_shape)\n",
    "    embeddings = embedding_layer(X_indices)\n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.6)(X)\n",
    "    X = LSTM(128, return_sequences=True)(X)\n",
    "    X = Dropout(0.6)(X)\n",
    "    X = LSTM(128)(X)\n",
    "    X = Dense(1, activation='sigmoid')(X)\n",
    "    model = Model(inputs=X_indices, outputs=X)\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "741f6178-900e-4b53-b857-bf7b768102f4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Model definition for a simple CNN\n",
    "def simple_cnn(emb):\n",
    "    model = Sequential()\n",
    "    model.add(emb)\n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6694307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(words,aspects):\n",
    "    vocab_len = len(words_to_index)+1\n",
    "    embed_vector_len = maxLen\n",
    "\n",
    "    embed_matrix = np.zeros((vocab_len, embed_vector_len))\n",
    "\n",
    "    for word, index in words_to_index.items():\n",
    "        embedding_vector = word_to_vec_map.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embed_matrix[index, :] = embedding_vector\n",
    "    input1 = Input(shape=(300,))\n",
    "    embedding_layer = Embedding(input_dim=vocab_len,\n",
    "                                output_dim=embed_vector_len,\n",
    "                                input_length=maxLen, weights = [embed_matrix],\n",
    "                                trainable=False)(input1)\n",
    "    \n",
    "    x1 = Conv1D(32,8,activation='relu')(embedding_layer)\n",
    "    x1 = MaxPooling1D(pool_size=4)(x1)\n",
    "    #x1 = Flatten()(x1)\n",
    "    \n",
    "    vocab_asp_len = len(aspect_to_index)+1\n",
    "    embed_vector_len = maxLen\n",
    "\n",
    "    embed_matrix = np.zeros((vocab_asp_len, embed_vector_len))\n",
    "\n",
    "    for word, index in aspect_to_index.items():\n",
    "        embedding_vector = word_to_vec_map.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embed_matrix[index, :] = embedding_vector\n",
    "    input2 = Input(shape=(300,))\n",
    "    aspect_embedding_layer = Embedding(input_dim=vocab_asp_len,\n",
    "                                output_dim=embed_vector_len,\n",
    "                                input_length=maxLen, weights = [embed_matrix],\n",
    "                                trainable=False)(input2)\n",
    "    x2 = Conv1D(32,8,activation='relu')(aspect_embedding_layer)\n",
    "    x2 = MaxPooling1D(pool_size=4)(x2)\n",
    "    #x2 = Flatten()(x2)\n",
    "    \n",
    "    concat = Concatenate()([x1,x2])\n",
    "    concat = Dense(64,activation='relu')(concat)\n",
    "    concat = Dense(32,activation='tanh')(concat)\n",
    "    #concat = GRU(16,activation='tanh',recurrent_activation='relu')(concat)\n",
    "    concat = MaxPooling1D(pool_size=2)(concat)\n",
    "    concat = Flatten()(concat)\n",
    "    concat = Dense(1,activation='sigmoid')(concat)\n",
    "    model = Model(inputs=[input1,input2],outputs=[concat])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "829562b4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 300, 300)     1131000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 300, 300)     1800        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 293, 32)      76832       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 293, 32)      76832       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 73, 32)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 73, 32)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 73, 64)       0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 73, 64)       4160        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 73, 32)       2080        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 36, 32)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1152)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1153        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,293,857\n",
      "Trainable params: 161,057\n",
      "Non-trainable params: 1,132,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = cnn(words_to_index,aspect_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "711e8796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "95/95 [==============================] - 14s 131ms/step - loss: 0.6339 - accuracy: 0.6685\n",
      "Epoch 2/10\n",
      "95/95 [==============================] - 13s 132ms/step - loss: 0.5817 - accuracy: 0.7266\n",
      "Epoch 3/10\n",
      "95/95 [==============================] - 14s 148ms/step - loss: 0.5784 - accuracy: 0.7251\n",
      "Epoch 4/10\n",
      "95/95 [==============================] - 16s 172ms/step - loss: 0.5716 - accuracy: 0.7173\n",
      "Epoch 5/10\n",
      "95/95 [==============================] - 16s 171ms/step - loss: 0.5381 - accuracy: 0.7292\n",
      "Epoch 6/10\n",
      "95/95 [==============================] - 16s 174ms/step - loss: 0.5089 - accuracy: 0.7371\n",
      "Epoch 7/10\n",
      "95/95 [==============================] - 16s 169ms/step - loss: 0.4559 - accuracy: 0.7643\n",
      "Epoch 8/10\n",
      "95/95 [==============================] - 16s 163ms/step - loss: 0.3913 - accuracy: 0.8245\n",
      "Epoch 9/10\n",
      "95/95 [==============================] - 17s 176ms/step - loss: 0.3510 - accuracy: 0.8449\n",
      "Epoch 10/10\n",
      "95/95 [==============================] - 17s 177ms/step - loss: 0.2991 - accuracy: 0.8802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbfdf8eb610>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adagrad = keras.optimizers.Adagrad(learning_rate = 0.01)\n",
    "\n",
    "model.compile(optimizer=adagrad, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=[X_train_indices,X_aspect_indices], y=Y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "64a22524-7574-4b4e-82e8-b251bdae2b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 300, 300)     1131000     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 300, 300)     1800        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 293, 32)      76832       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 293, 32)      76832       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 73, 32)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 73, 32)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 73, 64)       0           max_pooling1d_9[0][0]            \n",
      "                                                                 max_pooling1d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 73, 64)       4160        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 73, 32)       2080        dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 36, 32)       0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 1152)         0           max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            1153        flatten_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,293,857\n",
      "Trainable params: 161,057\n",
      "Non-trainable params: 1,132,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "95/95 [==============================] - 22s 204ms/step - loss: 0.6239 - acc: 0.6936 - f1_m: 0.7956 - precision_m: 0.7130 - recall_m: 0.9343\n",
      "Epoch 2/10\n",
      "95/95 [==============================] - 19s 204ms/step - loss: 0.5897 - acc: 0.7173 - f1_m: 0.8335 - precision_m: 0.7173 - recall_m: 1.0000\n",
      "Epoch 3/10\n",
      "95/95 [==============================] - 19s 196ms/step - loss: 0.5764 - acc: 0.7194 - f1_m: 0.8344 - precision_m: 0.7194 - recall_m: 1.0000\n",
      "Epoch 4/10\n",
      "95/95 [==============================] - 20s 207ms/step - loss: 0.5699 - acc: 0.7065 - f1_m: 0.8256 - precision_m: 0.7065 - recall_m: 1.0000\n",
      "Epoch 5/10\n",
      "95/95 [==============================] - 20s 209ms/step - loss: 0.5306 - acc: 0.7217 - f1_m: 0.8346 - precision_m: 0.7201 - recall_m: 0.9984\n",
      "Epoch 6/10\n",
      "95/95 [==============================] - 20s 207ms/step - loss: 0.4767 - acc: 0.7575 - f1_m: 0.8526 - precision_m: 0.7558 - recall_m: 0.9837\n",
      "Epoch 7/10\n",
      "95/95 [==============================] - 20s 207ms/step - loss: 0.4140 - acc: 0.8049 - f1_m: 0.8772 - precision_m: 0.8094 - recall_m: 0.9635\n",
      "Epoch 8/10\n",
      "95/95 [==============================] - 20s 206ms/step - loss: 0.3642 - acc: 0.8319 - f1_m: 0.8908 - precision_m: 0.8503 - recall_m: 0.9408\n",
      "Epoch 9/10\n",
      "95/95 [==============================] - 20s 211ms/step - loss: 0.3314 - acc: 0.8736 - f1_m: 0.9118 - precision_m: 0.8926 - recall_m: 0.9359\n",
      "Epoch 10/10\n",
      "95/95 [==============================] - 12s 127ms/step - loss: 0.2858 - acc: 0.8885 - f1_m: 0.9225 - precision_m: 0.9150 - recall_m: 0.9349\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "model1 = cnn(words_to_index,aspect_to_index)\n",
    "# compile the model\n",
    "model1.compile(optimizer=adagrad, loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "# fit the model\n",
    "model1.fit(x=[X_train_indices,X_aspect_indices], y=Y_train, batch_size=32, epochs=10)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy, f1_score, precision, recall = model1.evaluate([X_test_indices,X_test_aspect_indices], Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "84d108e8-c780-4f24-b109-1327ec181218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  0.3269771933555603\n",
      "accuracy :  0.8623435497283936\n",
      "f1_score :  0.9085442423820496\n",
      "precision :  0.873866856098175\n"
     ]
    }
   ],
   "source": [
    "print(\"loss : \" ,loss)\n",
    "print(\"accuracy : \",accuracy)\n",
    "print(\"f1_score : \",f1_score)\n",
    "print(\"precision : \",precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1335ccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formating test data so that we can use it\n",
    "X_test_indices = tokenizer1.texts_to_sequences(X_test)\n",
    "\n",
    "X_test_indices = pad_sequences(X_test_indices, maxlen=maxLen, padding='post')\n",
    "\n",
    "X_test_aspect_indices = tokenizer2.texts_to_sequences(X_aspect_test)\n",
    "\n",
    "X_test_aspect_indices = pad_sequences(X_test_aspect_indices, maxlen=maxLen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c003ac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 27ms/step - loss: 0.3683 - accuracy: 0.8407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3683435320854187, 0.8407281041145325]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([X_test_indices,X_test_aspect_indices], Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2ecb176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 2s 24ms/step - loss: 0.2989 - accuracy: 0.8907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29890453815460205, 0.890656054019928]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([X_train_indices,X_aspect_indices], Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e678d42f-abe5-4635-9ea2-d815fce26b1a",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 300)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 300, 300)          1131000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 300, 128)          219648    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 300, 128)          131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,613,945\n",
      "Trainable params: 482,945\n",
      "Non-trainable params: 1,131,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "''' Uncommenting out one line would run the function and here, the model is based on LSTM and CNN '''\n",
    "# model = simple_cnn(embedding_layer)\n",
    "model = LSTM_model(300)\n",
    "# model = CNN_LSTM_model(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47ea072b-c821-4d2b-89f9-9a3e9faf229a",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "48/48 [==============================] - 243s 5s/step - loss: 0.6785 - accuracy: 0.6893\n",
      "Epoch 2/5\n",
      "48/48 [==============================] - 244s 5s/step - loss: 0.5948 - accuracy: 0.7194\n",
      "Epoch 3/5\n",
      "48/48 [==============================] - 179s 4s/step - loss: 0.5858 - accuracy: 0.7289\n",
      "Epoch 4/5\n",
      "48/48 [==============================] - 188s 4s/step - loss: 0.5920 - accuracy: 0.7222\n",
      "Epoch 5/5\n",
      "48/48 [==============================] - 177s 3s/step - loss: 0.5915 - accuracy: 0.7224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbfdf8eba60>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the model\n",
    "'''\n",
    "adam = keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "\n",
    "'''\n",
    "adam = keras.optimizers.Adam(learning_rate = 0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x = X_train,y = Y_train, batch_size=64, epochs=5)\n",
    "model.fit(X_train_indices, Y_train, batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ed5efd2-445e-4595-b43f-a7b7f735626f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 264,  192,  902, ...,    0,    0,    0],\n",
       "       [ 122,   11, 1864, ...,    0,    0,    0],\n",
       "       [   1,   27,   49, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 360,    7,   25, ...,    0,    0,    0],\n",
       "       [ 360,    7,   25, ...,    0,    0,    0],\n",
       "       [2452,  380,  830, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Formating test data so that we can use it\n",
    "X_test_indices = tokenizer1.texts_to_sequences(X_test)\n",
    "\n",
    "X_test_indices = pad_sequences(X_test_indices, maxlen=maxLen, padding='post')\n",
    "\n",
    "X_test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5baa9f6-6cd3-4b4d-b7b4-114b219db83f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 14s 453ms/step - loss: 0.5666 - accuracy: 0.7474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5665537118911743, 0.7474402785301208]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation for test data\n",
    "model.evaluate(X_test_indices, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a470061-7a09-42e5-a5d9-e13d75b5577f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95/95 [==============================] - 35s 364ms/step - loss: 0.5911 - accuracy: 0.7220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5910611152648926, 0.722001314163208]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model evaluation for train data\n",
    "model.evaluate(X_train_indices, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8361ca3-a6c6-496b-ae64-cc0503fc228e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 300)          1131000   \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 293, 32)           76832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 146, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4672)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                46730     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,254,573\n",
      "Trainable params: 123,573\n",
      "Non-trainable params: 1,131,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "95/95 [==============================] - 9s 85ms/step - loss: 0.6171 - acc: 0.7153 - f1_m: 0.8241 - precision_m: 0.7249 - recall_m: 0.9704\n",
      "Epoch 2/10\n",
      "95/95 [==============================] - 8s 86ms/step - loss: 0.5560 - acc: 0.7224 - f1_m: 0.8358 - precision_m: 0.7223 - recall_m: 0.9983\n",
      "Epoch 3/10\n",
      "95/95 [==============================] - 8s 84ms/step - loss: 0.4974 - acc: 0.7429 - f1_m: 0.8434 - precision_m: 0.7426 - recall_m: 0.9816\n",
      "Epoch 4/10\n",
      "95/95 [==============================] - 8s 87ms/step - loss: 0.4279 - acc: 0.7865 - f1_m: 0.8661 - precision_m: 0.7927 - recall_m: 0.9616\n",
      "Epoch 5/10\n",
      "95/95 [==============================] - 8s 83ms/step - loss: 0.3711 - acc: 0.8356 - f1_m: 0.8933 - precision_m: 0.8393 - recall_m: 0.9596\n",
      "Epoch 6/10\n",
      "95/95 [==============================] - 8s 84ms/step - loss: 0.3437 - acc: 0.8413 - f1_m: 0.8868 - precision_m: 0.8475 - recall_m: 0.9337\n",
      "Epoch 7/10\n",
      "95/95 [==============================] - 8s 87ms/step - loss: 0.3176 - acc: 0.8549 - f1_m: 0.9026 - precision_m: 0.8676 - recall_m: 0.9477\n",
      "Epoch 8/10\n",
      "95/95 [==============================] - 8s 88ms/step - loss: 0.2718 - acc: 0.8877 - f1_m: 0.9244 - precision_m: 0.8949 - recall_m: 0.9596\n",
      "Epoch 9/10\n",
      "95/95 [==============================] - 8s 85ms/step - loss: 0.2477 - acc: 0.9003 - f1_m: 0.9307 - precision_m: 0.9079 - recall_m: 0.9588\n",
      "Epoch 10/10\n",
      "95/95 [==============================] - 12s 126ms/step - loss: 0.2111 - acc: 0.9210 - f1_m: 0.9458 - precision_m: 0.9311 - recall_m: 0.9647\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "model2 = simple_cnn(embedding_layer)\n",
    "# model2 = LSTM_model(300)\n",
    "# model2 = CNN_LSTM_model(embedding_layer)\n",
    "# compile the model\n",
    "model2.compile(optimizer=adagrad, loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "# fit the model\n",
    "model2.fit(x=X_train_indices, y=Y_train, batch_size=32, epochs=10)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy, f1_score, precision, recall = model2.evaluate(X_test_indices, Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6e59727-b3d0-4962-bfae-4cc92d9009f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  0.3304230272769928\n",
      "accuracy :  0.8577929735183716\n",
      "f1_score :  0.9024096727371216\n",
      "precision :  0.8718743324279785\n"
     ]
    }
   ],
   "source": [
    "print(\"loss : \" ,loss)\n",
    "print(\"accuracy : \",accuracy)\n",
    "print(\"f1_score : \",f1_score)\n",
    "print(\"precision : \",precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b20b3232-1c62-4350-97b1-a37b0f09f851",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aswin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "37b3d898-0003-4017-a312-3a9d7f6f09e8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create feature vectors\n",
    "vectorizer = TfidfVectorizer(min_df = 5,\n",
    "                             max_df = 0.8,\n",
    "                             sublinear_tf = True,\n",
    "                             use_idf = True)\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "test_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dc69cf44-d88e-4f44-91de-81c4c4ed96cd",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.271527s; Prediction time: 0.059065s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='sigmoid')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, Y_train)\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "# results\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "report = classification_report(Y_test, prediction_linear, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "753580ef-a77e-41f2-aaf7-6b64507adca8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# print('positive: ', report['1'])\n",
    "# print('negative: ', report['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "614c6223-20c9-4eaf-b1dc-e70cedbfc916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8361774744027304"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test,prediction_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3e2dead7-19aa-4405-8839-c0430766f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d918be18-4092-4094-9bf2-7aedab99240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = sbert_model.encode(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6bbc654b-6e0f-4d48-9e65-538321756557",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vec = sbert_model.encode(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "924a559f-a109-4c9b-bf66-e45f476820ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.238427s; Prediction time: 0.398030s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='rbf')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(sentence_embeddings, Y_train)\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(query_vec)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "# results\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "report = classification_report(Y_test, prediction_linear, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aa1c9ef0-9a82-4e74-8d1b-8eae537feb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.906712172923777"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test,prediction_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf9a06-6478-45bf-887e-afd4537616cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
