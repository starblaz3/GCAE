{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c913c04-a53f-414b-af43-60b9b4c2dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from keras.datasets import imdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import LSTM, Activation, Dropout, Dense, Input, Concatenate, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "import string\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten,MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb9d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File descriptors\n",
    "test_file = \"atsa_test.csv\"\n",
    "train_file = \"atsa_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683654e9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the bread is top notch as well.</td>\n",
       "      <td>bread</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have to say they have one of the fastest del...</td>\n",
       "      <td>delivery times</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food is always fresh and hot- ready to eat!</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>did i mention that the coffee is outstanding?</td>\n",
       "      <td>coffee</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>certainly not the best sushi in new york, howe...</td>\n",
       "      <td>sushi</td>\n",
       "      <td>conflict</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>creamy appetizers</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>warm pitas</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>taramasalata</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>eggplant salad</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>greek yogurt (with cuccumber, dill, and garlic)</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1134 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  \\\n",
       "0                       the bread is top notch as well.   \n",
       "1     i have to say they have one of the fastest del...   \n",
       "2           food is always fresh and hot- ready to eat!   \n",
       "3         did i mention that the coffee is outstanding?   \n",
       "4     certainly not the best sushi in new york, howe...   \n",
       "...                                                 ...   \n",
       "1129  creamy appetizers--taramasalata, eggplant sala...   \n",
       "1130  creamy appetizers--taramasalata, eggplant sala...   \n",
       "1131  creamy appetizers--taramasalata, eggplant sala...   \n",
       "1132  creamy appetizers--taramasalata, eggplant sala...   \n",
       "1133  creamy appetizers--taramasalata, eggplant sala...   \n",
       "\n",
       "                                               aspect sentiment  \n",
       "0                                               bread  positive  \n",
       "1                                      delivery times  positive  \n",
       "2                                                food  positive  \n",
       "3                                              coffee  positive  \n",
       "4                                               sushi  conflict  \n",
       "...                                               ...       ...  \n",
       "1129                                creamy appetizers  positive  \n",
       "1130                                       warm pitas   neutral  \n",
       "1131                                     taramasalata  positive  \n",
       "1132                                   eggplant salad  positive  \n",
       "1133  greek yogurt (with cuccumber, dill, and garlic)  positive  \n",
       "\n",
       "[1134 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre processing test data\n",
    "test_data = pd.read_csv(test_file)\n",
    "test_data['review'] = test_data['review'].astype(str)\n",
    "test_data['review'] = test_data['review'].str.lower()\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4a5e7fd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>but the waitstaff was so horrible to us.</td>\n",
       "      <td>waitstaff</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to be completely fair, the only redeeming fact...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the food is uniformly exceptional, with a very...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the food is uniformly exceptional, with a very...</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the food is uniformly exceptional, with a very...</td>\n",
       "      <td>menu</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3688</th>\n",
       "      <td>each table has a pot of boiling water sunken i...</td>\n",
       "      <td>pot of boiling water</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3689</th>\n",
       "      <td>each table has a pot of boiling water sunken i...</td>\n",
       "      <td>meats</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3690</th>\n",
       "      <td>each table has a pot of boiling water sunken i...</td>\n",
       "      <td>vegetables</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3691</th>\n",
       "      <td>each table has a pot of boiling water sunken i...</td>\n",
       "      <td>rice</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3692</th>\n",
       "      <td>each table has a pot of boiling water sunken i...</td>\n",
       "      <td>glass noodles</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3693 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review                aspect  \\\n",
       "0              but the waitstaff was so horrible to us.             waitstaff   \n",
       "1     to be completely fair, the only redeeming fact...                  food   \n",
       "2     the food is uniformly exceptional, with a very...                  food   \n",
       "3     the food is uniformly exceptional, with a very...               kitchen   \n",
       "4     the food is uniformly exceptional, with a very...                  menu   \n",
       "...                                                 ...                   ...   \n",
       "3688  each table has a pot of boiling water sunken i...  pot of boiling water   \n",
       "3689  each table has a pot of boiling water sunken i...                 meats   \n",
       "3690  each table has a pot of boiling water sunken i...            vegetables   \n",
       "3691  each table has a pot of boiling water sunken i...                  rice   \n",
       "3692  each table has a pot of boiling water sunken i...         glass noodles   \n",
       "\n",
       "     sentiment  \n",
       "0     negative  \n",
       "1     positive  \n",
       "2     positive  \n",
       "3     positive  \n",
       "4      neutral  \n",
       "...        ...  \n",
       "3688   neutral  \n",
       "3689   neutral  \n",
       "3690   neutral  \n",
       "3691   neutral  \n",
       "3692   neutral  \n",
       "\n",
       "[3693 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre processing train data\n",
    "train_data = pd.read_csv(train_file)\n",
    "train_data['review'] = train_data['review'].astype(str)\n",
    "train_data['review'] = train_data['review'].str.lower()\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc1adc6-05e5-4416-8656-0d51ddbc5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of stopwords\n",
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \n",
    "             \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n",
    "             \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \n",
    "             \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n",
    "             \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n",
    "             \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n",
    "             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\n",
    "             \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
    "             \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
    "             \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
    "             \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c244629-0124-4dbf-9d89-5d9a8f5f3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to process data using the stopwords list\n",
    "def remove_stopwords(data):\n",
    "    data['review without stopwords'] = data['review'].apply(lambda x : ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "    return data\n",
    "\n",
    "def remove_tags(string):\n",
    "    result = re.sub('<.*?>','',string)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53904607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-868cb5b0e9b9>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data_without_stopwords['clean_review'] = train_data_without_stopwords['clean_review'].str.replace('[{}]'.format(string.punctuation), ' ')\n"
     ]
    }
   ],
   "source": [
    "# Processing train data by removing stop words from reviews\n",
    "train_data_without_stopwords = remove_stopwords(train_data)\n",
    "train_data_without_stopwords['clean_review'] = train_data_without_stopwords['review without stopwords'].apply(lambda cw : remove_tags(cw))\n",
    "train_data_without_stopwords['clean_review'] = train_data_without_stopwords['clean_review'].str.replace('[{}]'.format(string.punctuation), ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af05d68-93e2-45a4-8c69-26a462fe99a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review without stopwords</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>but the waitstaff was so horrible to us.</td>\n",
       "      <td>waitstaff</td>\n",
       "      <td>negative</td>\n",
       "      <td>waitstaff horrible us.</td>\n",
       "      <td>waitstaff horrible us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to be completely fair, the only redeeming fact...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>completely fair, redeeming factor food, averag...</td>\n",
       "      <td>completely fair  redeeming factor food  averag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the food is uniformly exceptional, with a very...</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>food uniformly exceptional, capable kitchen wi...</td>\n",
       "      <td>food uniformly exceptional  capable kitchen wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the food is uniformly exceptional, with a very...</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>positive</td>\n",
       "      <td>food uniformly exceptional, capable kitchen wi...</td>\n",
       "      <td>food uniformly exceptional  capable kitchen wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the food is uniformly exceptional, with a very...</td>\n",
       "      <td>menu</td>\n",
       "      <td>neutral</td>\n",
       "      <td>food uniformly exceptional, capable kitchen wi...</td>\n",
       "      <td>food uniformly exceptional  capable kitchen wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3688</th>\n",
       "      <td>each table has a pot of boiling water sunken i...</td>\n",
       "      <td>pot of boiling water</td>\n",
       "      <td>neutral</td>\n",
       "      <td>table pot boiling water sunken surface, get pl...</td>\n",
       "      <td>table pot boiling water sunken surface  get pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3689</th>\n",
       "      <td>each table has a pot of boiling water sunken i...</td>\n",
       "      <td>meats</td>\n",
       "      <td>neutral</td>\n",
       "      <td>table pot boiling water sunken surface, get pl...</td>\n",
       "      <td>table pot boiling water sunken surface  get pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3690</th>\n",
       "      <td>each table has a pot of boiling water sunken i...</td>\n",
       "      <td>vegetables</td>\n",
       "      <td>neutral</td>\n",
       "      <td>table pot boiling water sunken surface, get pl...</td>\n",
       "      <td>table pot boiling water sunken surface  get pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3691</th>\n",
       "      <td>each table has a pot of boiling water sunken i...</td>\n",
       "      <td>rice</td>\n",
       "      <td>neutral</td>\n",
       "      <td>table pot boiling water sunken surface, get pl...</td>\n",
       "      <td>table pot boiling water sunken surface  get pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3692</th>\n",
       "      <td>each table has a pot of boiling water sunken i...</td>\n",
       "      <td>glass noodles</td>\n",
       "      <td>neutral</td>\n",
       "      <td>table pot boiling water sunken surface, get pl...</td>\n",
       "      <td>table pot boiling water sunken surface  get pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3693 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review                aspect  \\\n",
       "0              but the waitstaff was so horrible to us.             waitstaff   \n",
       "1     to be completely fair, the only redeeming fact...                  food   \n",
       "2     the food is uniformly exceptional, with a very...                  food   \n",
       "3     the food is uniformly exceptional, with a very...               kitchen   \n",
       "4     the food is uniformly exceptional, with a very...                  menu   \n",
       "...                                                 ...                   ...   \n",
       "3688  each table has a pot of boiling water sunken i...  pot of boiling water   \n",
       "3689  each table has a pot of boiling water sunken i...                 meats   \n",
       "3690  each table has a pot of boiling water sunken i...            vegetables   \n",
       "3691  each table has a pot of boiling water sunken i...                  rice   \n",
       "3692  each table has a pot of boiling water sunken i...         glass noodles   \n",
       "\n",
       "     sentiment                           review without stopwords  \\\n",
       "0     negative                             waitstaff horrible us.   \n",
       "1     positive  completely fair, redeeming factor food, averag...   \n",
       "2     positive  food uniformly exceptional, capable kitchen wi...   \n",
       "3     positive  food uniformly exceptional, capable kitchen wi...   \n",
       "4      neutral  food uniformly exceptional, capable kitchen wi...   \n",
       "...        ...                                                ...   \n",
       "3688   neutral  table pot boiling water sunken surface, get pl...   \n",
       "3689   neutral  table pot boiling water sunken surface, get pl...   \n",
       "3690   neutral  table pot boiling water sunken surface, get pl...   \n",
       "3691   neutral  table pot boiling water sunken surface, get pl...   \n",
       "3692   neutral  table pot boiling water sunken surface, get pl...   \n",
       "\n",
       "                                           clean_review  \n",
       "0                                waitstaff horrible us   \n",
       "1     completely fair  redeeming factor food  averag...  \n",
       "2     food uniformly exceptional  capable kitchen wi...  \n",
       "3     food uniformly exceptional  capable kitchen wi...  \n",
       "4     food uniformly exceptional  capable kitchen wi...  \n",
       "...                                                 ...  \n",
       "3688  table pot boiling water sunken surface  get pl...  \n",
       "3689  table pot boiling water sunken surface  get pl...  \n",
       "3690  table pot boiling water sunken surface  get pl...  \n",
       "3691  table pot boiling water sunken surface  get pl...  \n",
       "3692  table pot boiling water sunken surface  get pl...  \n",
       "\n",
       "[3693 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12968419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-04e680a37cae>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_data_without_stopwords['clean_review'] = test_data_without_stopwords['clean_review'].str.replace('[{}]'.format(string.punctuation), ' ')\n"
     ]
    }
   ],
   "source": [
    "# Processing test data by removing stop words from reviews\n",
    "test_data_without_stopwords = remove_stopwords(test_data)\n",
    "test_data_without_stopwords['clean_review']= test_data_without_stopwords['review without stopwords'].apply(lambda cw : remove_tags(cw))\n",
    "test_data_without_stopwords['clean_review'] = test_data_without_stopwords['clean_review'].str.replace('[{}]'.format(string.punctuation), ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24b4be42",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review without stopwords</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the bread is top notch as well.</td>\n",
       "      <td>bread</td>\n",
       "      <td>positive</td>\n",
       "      <td>bread top notch well.</td>\n",
       "      <td>bread top notch well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i have to say they have one of the fastest del...</td>\n",
       "      <td>delivery times</td>\n",
       "      <td>positive</td>\n",
       "      <td>say one fastest delivery times city.</td>\n",
       "      <td>say one fastest delivery times city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food is always fresh and hot- ready to eat!</td>\n",
       "      <td>food</td>\n",
       "      <td>positive</td>\n",
       "      <td>food always fresh hot- ready eat!</td>\n",
       "      <td>food always fresh hot  ready eat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>did i mention that the coffee is outstanding?</td>\n",
       "      <td>coffee</td>\n",
       "      <td>positive</td>\n",
       "      <td>mention coffee outstanding?</td>\n",
       "      <td>mention coffee outstanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>certainly not the best sushi in new york, howe...</td>\n",
       "      <td>sushi</td>\n",
       "      <td>conflict</td>\n",
       "      <td>certainly not best sushi new york, however, al...</td>\n",
       "      <td>certainly not best sushi new york  however  al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>creamy appetizers</td>\n",
       "      <td>positive</td>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>creamy appetizers  taramasalata  eggplant sala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>warm pitas</td>\n",
       "      <td>neutral</td>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>creamy appetizers  taramasalata  eggplant sala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>taramasalata</td>\n",
       "      <td>positive</td>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>creamy appetizers  taramasalata  eggplant sala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>eggplant salad</td>\n",
       "      <td>positive</td>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>creamy appetizers  taramasalata  eggplant sala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>greek yogurt (with cuccumber, dill, and garlic)</td>\n",
       "      <td>positive</td>\n",
       "      <td>creamy appetizers--taramasalata, eggplant sala...</td>\n",
       "      <td>creamy appetizers  taramasalata  eggplant sala...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1134 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  \\\n",
       "0                       the bread is top notch as well.   \n",
       "1     i have to say they have one of the fastest del...   \n",
       "2           food is always fresh and hot- ready to eat!   \n",
       "3         did i mention that the coffee is outstanding?   \n",
       "4     certainly not the best sushi in new york, howe...   \n",
       "...                                                 ...   \n",
       "1129  creamy appetizers--taramasalata, eggplant sala...   \n",
       "1130  creamy appetizers--taramasalata, eggplant sala...   \n",
       "1131  creamy appetizers--taramasalata, eggplant sala...   \n",
       "1132  creamy appetizers--taramasalata, eggplant sala...   \n",
       "1133  creamy appetizers--taramasalata, eggplant sala...   \n",
       "\n",
       "                                               aspect sentiment  \\\n",
       "0                                               bread  positive   \n",
       "1                                      delivery times  positive   \n",
       "2                                                food  positive   \n",
       "3                                              coffee  positive   \n",
       "4                                               sushi  conflict   \n",
       "...                                               ...       ...   \n",
       "1129                                creamy appetizers  positive   \n",
       "1130                                       warm pitas   neutral   \n",
       "1131                                     taramasalata  positive   \n",
       "1132                                   eggplant salad  positive   \n",
       "1133  greek yogurt (with cuccumber, dill, and garlic)  positive   \n",
       "\n",
       "                               review without stopwords  \\\n",
       "0                                 bread top notch well.   \n",
       "1                  say one fastest delivery times city.   \n",
       "2                     food always fresh hot- ready eat!   \n",
       "3                           mention coffee outstanding?   \n",
       "4     certainly not best sushi new york, however, al...   \n",
       "...                                                 ...   \n",
       "1129  creamy appetizers--taramasalata, eggplant sala...   \n",
       "1130  creamy appetizers--taramasalata, eggplant sala...   \n",
       "1131  creamy appetizers--taramasalata, eggplant sala...   \n",
       "1132  creamy appetizers--taramasalata, eggplant sala...   \n",
       "1133  creamy appetizers--taramasalata, eggplant sala...   \n",
       "\n",
       "                                           clean_review  \n",
       "0                                 bread top notch well   \n",
       "1                  say one fastest delivery times city   \n",
       "2                     food always fresh hot  ready eat   \n",
       "3                           mention coffee outstanding   \n",
       "4     certainly not best sushi new york  however  al...  \n",
       "...                                                 ...  \n",
       "1129  creamy appetizers  taramasalata  eggplant sala...  \n",
       "1130  creamy appetizers  taramasalata  eggplant sala...  \n",
       "1131  creamy appetizers  taramasalata  eggplant sala...  \n",
       "1132  creamy appetizers  taramasalata  eggplant sala...  \n",
       "1133  creamy appetizers  taramasalata  eggplant sala...  \n",
       "\n",
       "[1134 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "996868d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data into a list, seperating reviews from sentiment polarity\n",
    "train_reviews_list = []\n",
    "train_sentiment = []\n",
    "train_aspect = []\n",
    "for i in range(len(train_data_without_stopwords)):\n",
    "    train_reviews_list.append(train_data_without_stopwords.iloc[i,3])\n",
    "    train_sentiment.append(train_data_without_stopwords.loc[i,'sentiment'])\n",
    "    train_aspect.append(train_data_without_stopwords.loc[i,'aspect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a481913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data into a list, seperating reviews from sentiment polarity\n",
    "test_reviews_list = []\n",
    "test_sentiment = []\n",
    "test_aspect = []\n",
    "for i in range(len(test_data_without_stopwords)):\n",
    "    test_reviews_list.append(test_data_without_stopwords.iloc[i,3])\n",
    "    test_sentiment.append(test_data_without_stopwords.loc[i,'sentiment'])\n",
    "    test_aspect.append(test_data_without_stopwords.loc[i,'aspect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7739b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining train x and y values\n",
    "Y_train = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, train_sentiment)))\n",
    "X_train = train_reviews_list\n",
    "X_aspect_train = train_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e518b4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c62c4a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining train x and y values\n",
    "Y_test = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, test_sentiment)))\n",
    "X_test = test_reviews_list\n",
    "X_aspect_test = test_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f25c95b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3a9cd6a-e045-4560-8e76-359010f7f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing data\n",
    "tokenizer1 = Tokenizer(num_words=5000)\n",
    "tokenizer1.fit_on_texts(X_train)\n",
    "words_to_index = tokenizer1.word_index\n",
    "tokenizer2 = Tokenizer(num_words=5000)\n",
    "tokenizer2.fit_on_texts(X_aspect_train)\n",
    "aspect_to_index = tokenizer2.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16998371-fa77-49c9-89f6-87fb3312e2c6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to read the GloVe vectors for embedding\n",
    "def read_glove_vector(glove_vec):\n",
    "    with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            words_in_the_line = line.split()\n",
    "            current_word = words_in_the_line[0]\n",
    "            word_to_vec_map[current_word] = np.array(words_in_the_line[1:], dtype=np.float64)\n",
    "    return word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "384e0e11-b901-4a92-843e-bcc249e88140",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Loading the GloVe vectors\n",
    "word_to_vec_map = read_glove_vector('glove.6B.300d.txt')\n",
    "\n",
    "maxLen = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b8405b7-b36e-42bb-92fe-dd7ffca22921",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits :  3478  misses :  141\n",
      "hits :  1062  misses :  32\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "vocab_len = len(words_to_index)+1\n",
    "embed_vector_len = maxLen\n",
    "\n",
    "embed_matrix = np.zeros((vocab_len, embed_vector_len))\n",
    "hits = 0\n",
    "count = 0\n",
    "for word, index in words_to_index.items():\n",
    "    count+=1\n",
    "    embedding_vector = word_to_vec_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        hits+=1\n",
    "        embed_matrix[index, :] = embedding_vector\n",
    "print(\"hits : \", hits,\" misses : \",count-hits)\n",
    "embedding_layer = Embedding(input_dim=vocab_len,\n",
    "                            output_dim=embed_vector_len,\n",
    "                            input_length=maxLen, weights = [embed_matrix],\n",
    "                            trainable=False)\n",
    "\n",
    "vocab_asp_len = len(aspect_to_index)+1\n",
    "embed_vector_len = maxLen\n",
    "\n",
    "embed_matrix = np.zeros((vocab_asp_len, embed_vector_len))\n",
    "hits = 0\n",
    "count = 0\n",
    "for word, index in aspect_to_index.items():\n",
    "    count+=1\n",
    "    embedding_vector = word_to_vec_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        hits+=1\n",
    "        embed_matrix[index, :] = embedding_vector\n",
    "print(\"hits : \", hits,\" misses : \",count-hits)\n",
    "aspect_embedding_layer = Embedding(input_dim=vocab_asp_len,\n",
    "                            output_dim=embed_vector_len,\n",
    "                            input_length=maxLen, weights = [embed_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9937af70-cb37-4d5d-9dc8-d254d6dd8e26",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x7fe9abd10a30>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0b74113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x7fe9abd107c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspect_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f7455e1-cbef-4461-8117-ab1bfbc971e3",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_indices = tokenizer1.texts_to_sequences(X_train)\n",
    "\n",
    "X_train_indices = pad_sequences(X_train_indices, maxlen=maxLen, padding='post')\n",
    "\n",
    "X_aspect_indices = tokenizer2.texts_to_sequences(X_aspect_train)\n",
    "\n",
    "X_aspect_indices = pad_sequences(X_aspect_indices, maxlen=maxLen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcf10423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(words,aspects):\n",
    "    vocab_len = len(words_to_index)+1\n",
    "    embed_vector_len = maxLen\n",
    "\n",
    "    embed_matrix = np.zeros((vocab_len, embed_vector_len))\n",
    "\n",
    "    for word, index in words_to_index.items():\n",
    "        embedding_vector = word_to_vec_map.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embed_matrix[index, :] = embedding_vector\n",
    "    input1 = Input(shape=(300,))\n",
    "    embedding_layer = Embedding(input_dim=vocab_len,\n",
    "                                output_dim=embed_vector_len,\n",
    "                                input_length=maxLen, weights = [embed_matrix],\n",
    "                                trainable=False)(input1)\n",
    "    \n",
    "    x1 = Conv1D(32,8,activation='relu')(embedding_layer)\n",
    "    x1 = MaxPooling1D(pool_size=4)(x1)\n",
    "    #x1 = Flatten()(x1)\n",
    "    \n",
    "    vocab_asp_len = len(aspect_to_index)+1\n",
    "    embed_vector_len = maxLen\n",
    "\n",
    "    embed_matrix = np.zeros((vocab_asp_len, embed_vector_len))\n",
    "\n",
    "    for word, index in aspect_to_index.items():\n",
    "        embedding_vector = word_to_vec_map.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embed_matrix[index, :] = embedding_vector\n",
    "    input2 = Input(shape=(300,))\n",
    "    aspect_embedding_layer = Embedding(input_dim=vocab_asp_len,\n",
    "                                output_dim=embed_vector_len,\n",
    "                                input_length=maxLen, weights = [embed_matrix],\n",
    "                                trainable=False)(input2)\n",
    "    \n",
    "    x2 = Conv1D(32,8,activation='relu')(aspect_embedding_layer)\n",
    "    x2 = MaxPooling1D(pool_size=4)(x2)\n",
    "    #x2 = Flatten()(x2)\n",
    "    \n",
    "    concat = Concatenate()([x1,x2])\n",
    "    concat = Dense(64,activation='relu')(concat)\n",
    "    concat = Dense(32,activation='tanh')(concat)\n",
    "    #concat = GRU(16,activation='tanh',recurrent_activation='relu')(concat)\n",
    "    concat = MaxPooling1D(pool_size=2)(concat)\n",
    "    concat = Flatten()(concat)\n",
    "    concat = Dense(1,activation='sigmoid')(concat)\n",
    "    model = Model(inputs=[input1,input2],outputs=[concat])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "783eeb21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 300, 300)     1086000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 300, 300)     328500      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 293, 32)      76832       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 293, 32)      76832       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 73, 32)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 73, 32)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 73, 64)       0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 73, 64)       4160        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 73, 32)       2080        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 36, 32)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1152)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1153        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,575,557\n",
      "Trainable params: 161,057\n",
      "Non-trainable params: 1,414,500\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = cnn(words_to_index,aspect_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3723e2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "116/116 [==============================] - 15s 117ms/step - loss: 0.6799 - accuracy: 0.5894\n",
      "Epoch 2/10\n",
      "116/116 [==============================] - 13s 111ms/step - loss: 0.6701 - accuracy: 0.5920\n",
      "Epoch 3/10\n",
      "116/116 [==============================] - 13s 112ms/step - loss: 0.6593 - accuracy: 0.6057\n",
      "Epoch 4/10\n",
      "116/116 [==============================] - 14s 119ms/step - loss: 0.6207 - accuracy: 0.6651\n",
      "Epoch 5/10\n",
      "116/116 [==============================] - 13s 113ms/step - loss: 0.5506 - accuracy: 0.7340\n",
      "Epoch 6/10\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.4661 - accuracy: 0.7915\n",
      "Epoch 7/10\n",
      "116/116 [==============================] - 12s 105ms/step - loss: 0.4059 - accuracy: 0.8481\n",
      "Epoch 8/10\n",
      "116/116 [==============================] - 12s 107ms/step - loss: 0.3547 - accuracy: 0.8578\n",
      "Epoch 9/10\n",
      "116/116 [==============================] - 13s 108ms/step - loss: 0.3292 - accuracy: 0.8702\n",
      "Epoch 10/10\n",
      "116/116 [==============================] - 13s 114ms/step - loss: 0.2918 - accuracy: 0.8856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe9a9602a00>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adagrad = keras.optimizers.Adagrad(learning_rate = 0.01)\n",
    "\n",
    "model.compile(optimizer=adagrad, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x=[X_train_indices,X_aspect_indices], y=Y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98000bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formating test data so that we can use it\n",
    "X_test_indices = tokenizer1.texts_to_sequences(X_test)\n",
    "\n",
    "X_test_indices = pad_sequences(X_test_indices, maxlen=maxLen, padding='post')\n",
    "\n",
    "X_test_aspect_indices = tokenizer2.texts_to_sequences(X_aspect_test)\n",
    "\n",
    "X_test_aspect_indices = pad_sequences(X_test_aspect_indices, maxlen=maxLen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3a62aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4896 - accuracy: 0.7795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48963725566864014, 0.7795414328575134]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([X_test_indices,X_test_aspect_indices], Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67eb23b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 1s 11ms/step - loss: 0.2466 - accuracy: 0.9131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24655933678150177, 0.913078784942627]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([X_train_indices,X_aspect_indices], Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c467e029-890f-4f76-b5cf-6df0dd83ea90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 300, 300)     1086000     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 300, 300)     328500      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 293, 32)      76832       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 293, 32)      76832       embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 73, 32)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 73, 32)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 73, 64)       0           max_pooling1d_6[0][0]            \n",
      "                                                                 max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 73, 64)       4160        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 73, 32)       2080        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 36, 32)       0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1152)         0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            1153        flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,575,557\n",
      "Trainable params: 161,057\n",
      "Non-trainable params: 1,414,500\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "116/116 [==============================] - 21s 143ms/step - loss: 0.6822 - acc: 0.5676 - f1_m: 0.6970 - precision_m: 0.5933 - recall_m: 0.9100\n",
      "Epoch 2/10\n",
      "116/116 [==============================] - 14s 120ms/step - loss: 0.6694 - acc: 0.5877 - f1_m: 0.7362 - precision_m: 0.5870 - recall_m: 0.9993\n",
      "Epoch 3/10\n",
      "116/116 [==============================] - 18s 152ms/step - loss: 0.6558 - acc: 0.6138 - f1_m: 0.7384 - precision_m: 0.6061 - recall_m: 0.9584\n",
      "Epoch 4/10\n",
      "116/116 [==============================] - 24s 204ms/step - loss: 0.6159 - acc: 0.6700 - f1_m: 0.7618 - precision_m: 0.6564 - recall_m: 0.9225\n",
      "Epoch 5/10\n",
      "116/116 [==============================] - 23s 195ms/step - loss: 0.5593 - acc: 0.7211 - f1_m: 0.7856 - precision_m: 0.7222 - recall_m: 0.8740\n",
      "Epoch 6/10\n",
      "116/116 [==============================] - 24s 207ms/step - loss: 0.4816 - acc: 0.7927 - f1_m: 0.8300 - precision_m: 0.7914 - recall_m: 0.8800\n",
      "Epoch 7/10\n",
      "116/116 [==============================] - 25s 211ms/step - loss: 0.4234 - acc: 0.8193 - f1_m: 0.8478 - precision_m: 0.8270 - recall_m: 0.8805\n",
      "Epoch 8/10\n",
      "116/116 [==============================] - 24s 206ms/step - loss: 0.3707 - acc: 0.8601 - f1_m: 0.8795 - precision_m: 0.8749 - recall_m: 0.8907\n",
      "Epoch 9/10\n",
      "116/116 [==============================] - 24s 204ms/step - loss: 0.3276 - acc: 0.8773 - f1_m: 0.8925 - precision_m: 0.8915 - recall_m: 0.9019\n",
      "Epoch 10/10\n",
      "116/116 [==============================] - 24s 211ms/step - loss: 0.3016 - acc: 0.8869 - f1_m: 0.9021 - precision_m: 0.8962 - recall_m: 0.9147\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "model1 = cnn(words_to_index,aspect_to_index)\n",
    "# compile the model\n",
    "model1.compile(optimizer=adagrad, loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "# fit the model\n",
    "model1.fit(x=[X_train_indices,X_aspect_indices], y=Y_train, batch_size=32, epochs=10)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy, f1_score, precision, recall = model1.evaluate([X_test_indices,X_test_aspect_indices], Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a2201fb-06a7-4756-9b67-fb7c286890a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  0.4991653561592102\n",
      "accuracy :  0.7751322984695435\n",
      "f1_score :  0.8206244707107544\n",
      "precision :  0.7658053636550903\n"
     ]
    }
   ],
   "source": [
    "print(\"loss : \" ,loss)\n",
    "print(\"accuracy : \",accuracy)\n",
    "print(\"f1_score : \",f1_score)\n",
    "print(\"precision : \",precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741f6178-900e-4b53-b857-bf7b768102f4",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Model definition for a simple CNN\n",
    "# def test_cnn(emb,asp_emb):\n",
    "#     embedding_layer = Embedding(input_dim=vocab_len,\n",
    "#                             output_dim=embed_vector_len,\n",
    "#                             input_length=maxLen, weights = [embed_matrix],\n",
    "#                             trainable=False)\n",
    "#     model1 = Sequential()\n",
    "#     model1.add(emb)\n",
    "#     model1.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "#     model1.add(MaxPooling1D(pool_size=4))\n",
    "#     model1.add(Flatten())\n",
    "#     print(model1.summary())\n",
    "#     ##\n",
    "#     model2 = Sequential()\n",
    "#     model2.add(asp_emb)\n",
    "#     model2.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "#     model2.add(MaxPooling1D(pool_size=4))\n",
    "#     model2.add(Flatten())\n",
    "#     print(model2.summary())\n",
    "#     concat = Concatenate([model1,model2])\n",
    "#     # ADD GTRU\n",
    "#     ##\n",
    "#     '''\n",
    "#     model3.add(MaxPooling1D(pool_size=2))\n",
    "#     model3.add(Flatten())\n",
    "#     model3.add(Dense(10, activation='relu'))\n",
    "#     model3.add(Dense(1, activation='sigmoid'))\n",
    "#     print(model3.summary())\n",
    "#     return model3\n",
    "#     x = activation(\"Relu\")\n",
    "    \n",
    "#     x = activation(\"Tanh\")\n",
    "#     concat = MaxPooling1D(pool_size=2)(concat)\n",
    "#     concat = Flatten()(concat)\n",
    "#     concat = Dense(10,activation='relu')(concat)\n",
    "#     concat = Dense(1,activation='relu')(concat)\n",
    "#     '''\n",
    "#     model = Model(inputs=[concat],outputs=[concat])\n",
    "#     print(model.summary())\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243299f6-13a5-4213-9a50-0f8b908d5284",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Model definition for a LSTM based model\n",
    "# def LSTM_model(input_shape):\n",
    "#     X_indices = Input(input_shape)\n",
    "#     embeddings = embedding_layer(X_indices)\n",
    "#     X = LSTM(128, return_sequences=True)(embeddings)\n",
    "#     X = Dropout(0.6)(X)\n",
    "#     X = LSTM(128, return_sequences=True)(X)\n",
    "#     X = Dropout(0.6)(X)\n",
    "#     X = LSTM(128)(X)\n",
    "#     X = Dense(1, activation='sigmoid')(X)\n",
    "#     model = Model(inputs=X_indices, outputs=X)\n",
    "#     print(model.summary())\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ad259-c204-4690-8c3c-f927057c26ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17340297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def CNN_LSTM_model(emb):\n",
    "#     embedding_vecor_length = maxLen\n",
    "#     model = Sequential()\n",
    "#     model.add(emb)\n",
    "#     model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(LSTM(256))\n",
    "#     model.add(Dense(128, activation='sigmoid'))\n",
    "#     model.add(Dense(64, activation='sigmoid'))\n",
    "#     model.add(Dense(1, activation='softmax'))\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e678d42f-abe5-4635-9ea2-d815fce26b1a",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# ''' Uncommenting out one line would run the function and here, the model is based on LSTM and CNN '''\n",
    "# # model = test_cnn(embedding_layer, aspect_embedding_layer)\n",
    "# model = LSTM_model(768)\n",
    "# # model = CNN_LSTM_model(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ea072b-c821-4d2b-89f9-9a3e9faf229a",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Running the model\n",
    "# '''\n",
    "# adam = keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "\n",
    "# '''\n",
    "# adam = keras.optimizers.Adam(learning_rate = 0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False)\n",
    "\n",
    "# model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # model.fit(X_train_indices, Y_train, batch_size=64, epochs=5)\n",
    "# model.fit(sentence_embeddings, Y_train, batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5efd2-445e-4595-b43f-a7b7f735626f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Formating test data so that we can use it\n",
    "# X_test_indices = tokenizer1.texts_to_sequences(X_test)\n",
    "\n",
    "# X_test_indices = pad_sequences(X_test_indices, maxlen=maxLen, padding='post')\n",
    "\n",
    "# X_test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5baa9f6-6cd3-4b4d-b7b4-114b219db83f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model evaluation for test data\n",
    "# model.evaluate(X_test_indices, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a470061-7a09-42e5-a5d9-e13d75b5577f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model evaluation for train data\n",
    "# model.evaluate(X_train_indices, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8361ca3-a6c6-496b-ae64-cc0503fc228e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/aswin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc69cf44-d88e-4f44-91de-81c4c4ed96cd",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create feature vectors\n",
    "vectorizer = TfidfVectorizer(min_df = 5,\n",
    "                             max_df = 0.8,\n",
    "                             sublinear_tf = True,\n",
    "                             use_idf = True)\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "test_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6a707-fcee-47df-9007-d8bd8c466925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "753580ef-a77e-41f2-aaf7-6b64507adca8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.632158s; Prediction time: 0.321239s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, Y_train)\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "# results\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "report = classification_report(Y_test, prediction_linear, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c409a65a-d6dc-4ffd-bdf7-6961aa4c15c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive:  {'precision': 0.8021248339973439, 'recall': 0.8296703296703297, 'f1-score': 0.8156650911546253, 'support': 728}\n",
      "negative:  {'precision': 0.6745406824146981, 'recall': 0.6330049261083743, 'f1-score': 0.653113087674714, 'support': 406}\n"
     ]
    }
   ],
   "source": [
    "print('positive: ', report['1'])\n",
    "print('negative: ', report['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f871e28-2889-4ffe-85df-e45f975b95f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3693"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40cc04dc-a3e9-4caa-b140-dbc1776e7f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7592592592592593"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test,prediction_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "813f0b8b-ebbc-4ca0-aad7-875105853ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (2.1.0)\n",
      "Requirement already satisfied: nltk in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from sentence-transformers) (3.6.5)\n",
      "Requirement already satisfied: scikit-learn in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: tokenizers>=0.10.3 in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from sentence-transformers) (0.10.3)\n",
      "Requirement already satisfied: sentencepiece in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from sentence-transformers) (0.1.96)\n",
      "Requirement already satisfied: scipy in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from sentence-transformers) (1.6.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from sentence-transformers) (0.2.1)\n",
      "Requirement already satisfied: torchvision in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from sentence-transformers) (0.8.0a0)\n",
      "Requirement already satisfied: tqdm in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from sentence-transformers) (4.62.3)\n",
      "Requirement already satisfied: numpy in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from sentence-transformers) (1.19.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from sentence-transformers) (4.12.5)\n",
      "Requirement already satisfied: typing-extensions in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.8.3)\n",
      "Requirement already satisfied: filelock in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: sacremoses in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.6)\n",
      "Requirement already satisfied: click in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from nltk->sentence-transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from nltk->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: six in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/aswin/anaconda3/envs/ml/lib/python3.9/site-packages (from torchvision->sentence-transformers) (8.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12b631c5-e432-48cc-a52d-7f1c87f5d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5944abe7-72e8-4f13-9a10-780e18fe099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = sbert_model.encode(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a6548c98-4b61-4145-8112-e4bbb5b07a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3693"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9215b7ed-a460-4a62-9c4a-263127205038",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.34444696e-01,  6.47295833e-01,  1.86367583e+00,  1.16021030e-01,\n",
       "       -1.98262960e-01,  7.22987771e-01,  1.37537980e+00,  5.59267998e-01,\n",
       "        3.41259331e-01, -2.56377518e-01, -1.29588282e+00,  3.47438246e-01,\n",
       "        4.10151482e-01,  2.38022372e-01, -1.63080722e-01,  5.38577139e-01,\n",
       "       -3.50794792e-01, -6.85997307e-01,  2.90022790e-01, -9.21145916e-01,\n",
       "        4.12595004e-01, -3.63073796e-01,  1.18249321e+00, -6.41812980e-01,\n",
       "       -1.45813480e-01,  5.96415885e-02,  1.23967035e-02, -1.40196776e+00,\n",
       "       -1.20230460e+00,  1.45145491e-01,  1.60908565e-01, -6.85921133e-01,\n",
       "        6.51957810e-01, -2.02278748e-01, -3.32574606e-01,  5.61232686e-01,\n",
       "       -9.10916179e-03,  6.21988289e-02,  1.46808401e-01,  3.51528913e-01,\n",
       "        9.72895086e-01,  4.83499199e-01,  6.67923510e-01,  4.43095148e-01,\n",
       "       -1.51802540e-01,  5.94743602e-02,  1.21916282e+00,  2.90922940e-01,\n",
       "        8.32786143e-01, -1.08085716e+00, -9.73090172e-01,  8.80242661e-02,\n",
       "        1.77330136e+00,  7.10279107e-01, -4.22383726e-01, -9.49867070e-01,\n",
       "        5.24195507e-02, -4.74535406e-01, -4.41346705e-01, -2.23466888e-01,\n",
       "       -6.37578070e-01,  3.13812256e-01,  1.07853971e-01,  5.75411916e-01,\n",
       "       -6.37753010e-01,  1.02014709e+00, -8.00351575e-02, -4.21842605e-01,\n",
       "       -4.61084098e-01, -3.61073226e-01,  1.14977360e+00, -1.60941720e+00,\n",
       "       -2.05846295e-01, -3.62479687e-02, -5.41133285e-01, -7.56238520e-01,\n",
       "       -4.08414036e-01,  5.25515914e-01,  4.95543480e-01,  1.28793263e+00,\n",
       "        5.73752820e-02,  1.10976316e-01,  8.82592022e-01,  2.17606081e-03,\n",
       "       -2.71616764e-02,  4.91452366e-01,  1.03440225e+00, -4.32603471e-02,\n",
       "       -1.58292794e+00, -2.43108300e-03,  5.46936452e-01,  8.67065132e-01,\n",
       "        1.19683993e+00, -8.08921635e-01, -3.05388212e-01, -8.41922104e-01,\n",
       "       -3.92083302e-02,  5.21513104e-01,  3.31784874e-01, -1.20790049e-01,\n",
       "       -1.09612405e+00,  2.28359655e-01,  8.36987972e-01,  4.27429341e-02,\n",
       "       -1.92569289e-02, -2.28553191e-01,  1.03848919e-01,  2.85322428e-01,\n",
       "       -6.69197217e-02,  2.85966456e-01, -1.90654039e-01,  1.02046561e+00,\n",
       "        3.25754851e-01, -4.82395828e-01,  1.68763250e-01,  3.61302525e-01,\n",
       "       -1.64382160e+00, -1.08073875e-01, -2.34555051e-01,  4.24129486e-01,\n",
       "        1.43315709e+00,  1.20099731e-01,  4.52274472e-01, -4.33485687e-01,\n",
       "        2.18736157e-01, -1.88535005e-01,  9.99342263e-01, -4.27831233e-01,\n",
       "        1.93281546e-01,  1.67133778e-01, -5.81741869e-01,  1.27023947e+00,\n",
       "       -6.55022919e-01,  3.92012537e-01, -1.85450837e-01, -1.63347647e-01,\n",
       "        4.45158668e-02, -5.75963140e-01,  6.18305743e-01,  4.27251235e-02,\n",
       "       -7.97564507e-01,  3.64463955e-01, -3.16456616e-01,  4.27302837e-01,\n",
       "       -4.84343082e-01,  1.23321041e-01,  1.07732214e-01, -5.00160977e-02,\n",
       "       -3.19562435e-01, -2.37840652e-01, -8.71147752e-01,  4.56984043e-02,\n",
       "       -5.12438118e-01,  2.06598207e-01, -2.54951686e-01, -1.29604697e+00,\n",
       "        1.06515193e+00,  3.16364318e-03, -5.44398487e-01, -1.02760874e-01,\n",
       "        3.96397412e-01,  2.39100143e-01, -3.61362696e-01,  9.68707919e-01,\n",
       "        4.64264393e-01, -4.26527023e-01, -7.51852989e-01, -6.90270126e-01,\n",
       "        5.14266610e-01, -2.96316445e-01,  2.99682379e-01, -5.26135564e-01,\n",
       "        2.56761104e-01, -2.29449183e-01,  4.25133973e-01,  5.70406556e-01,\n",
       "       -6.82529569e-01,  1.64109096e-01,  3.65662336e-01, -6.75247908e-02,\n",
       "       -6.02915548e-02,  3.20192100e-03, -3.22103679e-01, -1.54925659e-01,\n",
       "        5.52808866e-02, -8.23487937e-01,  5.90960920e-01, -5.68371415e-01,\n",
       "       -1.20774999e-01,  1.82719037e-01, -6.68706000e-01, -1.15847850e+00,\n",
       "        1.28223634e+00, -8.83996189e-01,  5.43891788e-02, -1.62957590e-02,\n",
       "        1.01080947e-01, -4.40481395e-01,  7.86563307e-02,  5.74708760e-01,\n",
       "        1.94620296e-01, -7.73216337e-02, -2.03796357e-01,  4.32348065e-02,\n",
       "        4.07314360e-01,  1.11728740e+00, -7.61670053e-01, -1.51351169e-01,\n",
       "       -2.78280109e-01, -4.82581437e-01, -7.96122432e-01, -3.42454225e-01,\n",
       "        4.44852971e-02,  6.06967866e-01,  6.37702584e-01, -8.17926705e-01,\n",
       "        5.75837016e-01, -1.07389247e+00, -2.54823655e-01, -1.60144661e-02,\n",
       "        3.90810162e-01,  5.29359914e-02,  3.56508940e-01,  8.61993432e-01,\n",
       "       -8.83578420e-01,  1.15422234e-01, -1.20134912e-01,  8.26870501e-01,\n",
       "       -1.47015035e+00, -1.87762827e-01,  4.86826718e-01,  2.06869394e-01,\n",
       "       -1.00844920e+00,  5.83378002e-02,  3.19541469e-02, -1.08670580e+00,\n",
       "       -3.55748199e-02, -4.26757634e-01,  1.07960069e+00, -1.60597846e-01,\n",
       "        2.54949808e-01,  7.62161851e-01,  9.79168475e-01,  1.17631626e+00,\n",
       "       -9.46211338e-01, -8.27599645e-01,  5.98009646e-01, -5.58669746e-01,\n",
       "       -5.28163016e-01,  1.10556793e+00, -2.37935379e-01, -9.73133862e-01,\n",
       "        3.81653845e-01, -3.11551511e-01,  1.33609341e-03, -1.07876427e-01,\n",
       "        4.80396450e-02,  5.63743711e-01,  6.28213048e-01, -7.35378802e-01,\n",
       "       -4.40390199e-01,  5.89418471e-01, -1.21382272e+00,  2.18438402e-01,\n",
       "        5.32640636e-01,  7.67609060e-01, -8.79949868e-01,  2.36312792e-01,\n",
       "       -5.61370552e-01,  6.21087134e-01, -1.52727468e-02, -1.19328153e+00,\n",
       "       -2.45298699e-01,  2.71779209e-01, -7.36721516e-01, -5.82729816e-01,\n",
       "        7.67705321e-01, -4.14670378e-01,  2.98223317e-01,  1.62936524e-01,\n",
       "        5.19582303e-03,  1.30740440e+00, -1.38469625e+00,  3.82436007e-01,\n",
       "       -8.73802960e-01,  5.49680829e-01,  1.08856335e-01, -4.28887188e-01,\n",
       "       -5.54338098e-01, -2.29775831e-01, -6.05558276e-01,  1.10221672e+00,\n",
       "       -1.99796408e-01, -7.55475640e-01,  1.00096345e+00, -1.65914130e+00,\n",
       "        7.48901889e-02, -4.71842676e-01, -4.14587468e-01,  3.89754146e-01,\n",
       "       -5.91138005e-02,  2.19396979e-01, -4.98161048e-01,  3.99432927e-01,\n",
       "       -4.95750576e-01, -2.36365408e-01,  2.90452629e-01,  5.01577973e-01,\n",
       "       -7.95068741e-01, -4.61222559e-01, -7.52778649e-01, -5.70664406e-02,\n",
       "       -4.72224861e-01, -1.63405180e-01,  1.45820928e+00, -1.91370323e-01,\n",
       "       -5.69929600e-01,  2.27099821e-01,  1.08743019e-01, -4.87230569e-01,\n",
       "       -7.27022737e-02,  1.49769396e-01,  2.37847641e-01,  1.46834448e-03,\n",
       "       -4.52532917e-01, -1.05917394e+00,  4.70715940e-01,  2.81159759e-01,\n",
       "        2.12779149e-01, -8.02010953e-01, -2.53999770e-01, -7.80634105e-01,\n",
       "        4.29391325e-01,  5.19616306e-01,  2.19690368e-01,  1.15060937e+00,\n",
       "       -8.53082955e-01, -4.29255255e-02,  2.99433023e-01,  5.10707378e-01,\n",
       "        3.38606626e-01, -1.03219378e+00, -6.10598087e-01, -1.41226494e+00,\n",
       "        8.55558395e-01, -2.88429320e-01, -7.68057048e-01, -4.92363214e-01,\n",
       "        1.79133624e-01,  1.01198837e-01,  1.62721708e-01,  6.26554862e-02,\n",
       "       -4.57808673e-01,  3.16677541e-01, -8.87733936e-01, -1.33069050e+00,\n",
       "       -1.68043688e-01,  2.84354150e-01,  1.55417725e-01,  2.35530045e-02,\n",
       "        3.76886368e-01,  8.19151521e-01, -6.54725671e-01, -7.35756040e-01,\n",
       "        2.61693865e-01,  3.60466875e-02, -7.29965493e-02,  2.14100316e-01,\n",
       "        1.19267857e+00,  4.54212874e-01, -7.30663165e-02,  8.01011086e-01,\n",
       "       -4.21979606e-01, -1.71912760e-02,  2.70231426e-01, -6.16596341e-01,\n",
       "       -8.71855840e-02, -8.93892884e-01, -7.01427758e-01, -1.65450799e+00,\n",
       "        7.78289676e-01, -9.32287872e-01,  5.50329328e-01,  1.49121121e-01,\n",
       "        2.47895941e-01,  2.55183011e-01,  6.27930522e-01, -7.55307853e-01,\n",
       "        5.40099621e-01,  2.25458182e-02,  1.67328072e+00,  4.77612346e-01,\n",
       "        3.86388637e-02, -6.31204545e-01, -8.86970997e-01, -1.26183522e+00,\n",
       "        7.98171639e-01,  8.59313369e-01,  6.79193318e-01,  2.53056169e-01,\n",
       "        8.41252804e-02,  4.69498903e-01,  2.80429184e-01,  6.54421329e-01,\n",
       "        1.10318625e+00,  9.29552019e-01, -7.72038177e-02,  2.65642285e-01,\n",
       "       -1.07630682e+00, -6.59066319e-01,  5.63707232e-01,  4.66130883e-01,\n",
       "       -9.60007787e-01,  3.46817970e-01, -2.24625438e-01,  4.74364668e-01,\n",
       "        4.74821404e-02, -9.97516096e-01,  9.16107774e-01,  1.09338915e+00,\n",
       "       -7.83191323e-01,  2.11052060e-01, -3.46312486e-02,  3.62968922e-01,\n",
       "        1.61062274e-02,  8.96215260e-01, -2.87128627e-01,  4.65910174e-02,\n",
       "       -5.32189429e-01, -7.71452367e-01, -4.47957307e-01,  2.17533514e-01,\n",
       "       -4.34285223e-01,  1.43068746e-01,  3.11685771e-01, -8.08482945e-01,\n",
       "        2.05428414e-02, -9.48175073e-01,  8.71505857e-01, -2.95164287e-02,\n",
       "        1.07138240e+00, -1.36769295e+00,  7.16592789e-01,  4.73251909e-01,\n",
       "       -6.71301782e-01, -2.46763304e-01, -6.12739623e-01, -2.85595715e-01,\n",
       "        5.84693491e-01, -3.26521881e-02,  4.02542114e-01,  2.20325783e-01,\n",
       "        2.71019548e-01,  1.04958761e+00,  1.03008533e+00, -2.40842000e-01,\n",
       "       -5.39591834e-02,  1.49133012e-01, -4.38895047e-01,  1.80573575e-02,\n",
       "        2.11750135e-01,  8.34110260e-01, -9.47828472e-01, -6.58324897e-01,\n",
       "       -2.29813829e-01,  2.24180385e-01, -6.01648271e-01, -7.13732421e-01,\n",
       "       -7.84826875e-02, -3.76476079e-01, -9.55879986e-01, -3.07809591e-01,\n",
       "        3.40799600e-01, -7.32608885e-02, -1.76761284e-01, -3.79043818e-01,\n",
       "       -1.57102144e+00,  6.53429806e-01, -2.25136846e-01,  1.52322233e+00,\n",
       "        8.40834439e-01,  6.09273255e-01, -1.70166716e-01, -5.13207316e-01,\n",
       "       -5.44205487e-01,  2.61077195e-01,  1.93856642e-01,  1.03082287e+00,\n",
       "        4.00236547e-01,  1.91184312e-01,  7.01231182e-01,  3.18560928e-01,\n",
       "       -1.38961697e+00, -4.75047261e-01, -3.82973582e-01, -7.77486920e-01,\n",
       "       -7.03217462e-02,  2.79273540e-01, -3.20853412e-01, -4.20681946e-02,\n",
       "        1.02789891e+00,  7.42417634e-01, -1.80422902e-01, -4.47595805e-01,\n",
       "        5.81469893e-01,  1.72388673e-01,  3.04419935e-01, -1.93675429e-01,\n",
       "        7.67352104e-01,  6.97984636e-01, -5.67984819e-01, -2.48928368e-03,\n",
       "       -4.89788294e-01, -5.68323672e-01, -3.03795427e-01, -1.24348931e-01,\n",
       "       -3.55782896e-01, -1.09173596e-01, -4.78939563e-01, -1.58333793e-01,\n",
       "       -1.21800518e+00,  6.62915289e-01, -1.42692551e-01, -4.87468094e-01,\n",
       "        4.14945304e-01, -2.45435759e-01, -1.12057991e-01,  7.14122117e-01,\n",
       "       -1.37899250e-01,  4.84033912e-01, -7.74966538e-01, -2.61591971e-01,\n",
       "       -7.19092965e-01,  1.00891042e+00,  4.12538499e-01, -2.38849074e-01,\n",
       "       -2.64184147e-01, -2.76200384e-01,  1.20474510e-01,  9.00675356e-01,\n",
       "        7.91984200e-01,  4.60266799e-01,  1.23210537e+00, -6.42371595e-01,\n",
       "       -1.49638736e+00,  4.29299444e-01,  5.94157517e-01, -1.99683949e-01,\n",
       "       -6.05861545e-01, -2.49868989e-01,  4.40237194e-01,  4.00886059e-01,\n",
       "        4.71693695e-01, -1.47865430e-01, -6.08364411e-04, -8.28985393e-01,\n",
       "        4.46892947e-01,  1.91048712e-01, -3.55020672e-01,  2.94426717e-02,\n",
       "        3.75210717e-02,  3.04910421e-01, -6.36870088e-03,  2.51057476e-01,\n",
       "        2.05763001e-02, -6.91952407e-01,  1.67004958e-01, -1.84840128e-01,\n",
       "       -9.36029434e-01,  1.28470913e-01,  9.93815303e-01, -5.30103862e-01,\n",
       "        1.34537184e+00,  1.23400497e+00, -3.81960094e-01,  1.29607663e-01,\n",
       "        3.51054996e-01, -3.93961221e-01, -7.25873291e-01,  4.57145840e-01,\n",
       "       -8.35322365e-02,  6.09385669e-01, -7.71856308e-02, -5.74744642e-01,\n",
       "       -9.17255044e-01, -3.51573765e-01,  8.39038908e-01, -6.70444548e-01,\n",
       "        3.74649167e-02, -1.16589062e-01,  1.70232151e-02, -1.03734657e-01,\n",
       "        5.94695628e-01, -3.24460238e-01,  1.39360678e+00, -1.88679963e-01,\n",
       "       -3.35471541e-01,  2.51616806e-01,  1.70962408e-01, -1.35244727e-01,\n",
       "       -3.10307920e-01,  2.24495202e-01, -9.02454019e-01,  6.54230118e-01,\n",
       "       -4.99212183e-02,  1.31330624e-01, -4.20394838e-01, -1.74461249e-02,\n",
       "       -8.78984988e-01, -8.29293072e-01,  5.33314764e-01, -9.06326294e-01,\n",
       "        8.93200636e-02,  2.32496373e-02, -8.23054194e-01, -1.02305114e+00,\n",
       "       -1.09486711e+00,  1.41574576e-01,  1.13994531e-01,  1.51661038e-01,\n",
       "        4.47508961e-01, -7.00567544e-01,  1.52621523e-01, -9.40201506e-02,\n",
       "        2.89789408e-01, -1.20603576e-01,  2.85281569e-01, -5.60297966e-01,\n",
       "        6.49795473e-01, -2.86318094e-01,  6.73272133e-01,  2.39576653e-01,\n",
       "        7.07908452e-01, -8.34097809e-05, -9.39661860e-02,  1.08082402e+00,\n",
       "        7.71782398e-01,  8.39426890e-02, -2.83442233e-02, -4.66464460e-01,\n",
       "       -2.65583605e-01, -3.73884916e-01,  3.43112856e-01, -4.87650186e-01,\n",
       "        1.64675549e-01, -1.36217272e+00,  1.61115840e-01, -3.32413197e-01,\n",
       "        4.57409561e-01, -4.00108518e-03,  3.57354015e-01,  4.46745753e-03,\n",
       "       -3.10421288e-01,  2.33068436e-01,  2.66338773e-02, -1.83550432e-01,\n",
       "       -8.57458040e-02,  3.47316951e-01, -1.15495801e+00, -1.87502608e-01,\n",
       "        2.35843554e-01, -8.10238838e-01,  1.86300278e-01,  1.26053318e-01,\n",
       "        2.63002485e-01,  7.62043834e-01,  2.65613962e-02, -3.74657027e-02,\n",
       "       -5.73038876e-01,  3.12964201e-01, -4.64993328e-01,  3.59719306e-01,\n",
       "       -5.28236553e-02, -1.50735188e+00,  3.50268692e-01, -1.42928827e+00,\n",
       "        1.41289425e+00, -4.02836859e-01, -7.70929782e-03, -6.63917065e-01,\n",
       "        1.72150031e-01, -2.24075153e-01, -2.17138082e-01, -1.11995742e-01,\n",
       "        1.52670264e+00, -4.37001079e-01,  4.51974362e-01,  4.76863176e-01,\n",
       "        1.74556375e-02,  8.02446067e-01,  1.00214970e+00, -4.11807567e-01,\n",
       "       -7.75690079e-01, -8.33566964e-01, -4.90074217e-01, -2.05949306e-01,\n",
       "       -8.80101323e-02,  1.47229999e-01,  1.00712128e-01,  2.70233214e-01,\n",
       "       -1.14048219e+00, -3.19774836e-01, -4.84554440e-01,  6.57429576e-01,\n",
       "       -4.61837262e-01, -4.63435143e-01, -1.21522343e+00,  1.54745251e-01,\n",
       "        4.46001679e-01,  5.09389520e-01, -3.95001210e-02,  1.33348954e+00,\n",
       "       -5.33383012e-01,  2.33471822e-02, -9.42679763e-01,  2.20903978e-01,\n",
       "        2.10699573e-01, -7.31519222e-01,  3.92158836e-01,  2.22821921e-01,\n",
       "       -1.03402205e-01,  3.01924199e-01, -1.58727720e-01, -1.60529748e-01,\n",
       "        3.30396801e-01, -2.65888423e-01, -2.97253758e-01,  1.38321156e-02,\n",
       "       -5.41198075e-01, -8.30390036e-01,  7.02269018e-01,  2.71149635e-01,\n",
       "       -3.35302129e-02, -5.89563429e-01,  7.46023834e-01,  1.68243438e-01,\n",
       "        9.34344709e-01, -1.60957411e-01, -2.46072561e-01, -1.90712512e-02,\n",
       "        4.36451524e-01,  2.04387113e-01,  4.02720779e-01, -8.84203136e-01,\n",
       "        1.56685002e-02,  3.04761827e-01,  4.82252687e-01, -2.48656958e-01,\n",
       "       -4.58261281e-01,  1.95052162e-01, -2.39355899e-02,  6.37488365e-01,\n",
       "       -5.93726262e-02,  6.40743151e-02,  1.02740109e-01, -1.31785482e-01,\n",
       "       -1.34862423e+00, -2.45396107e-01,  2.88351238e-01, -1.06838502e-01,\n",
       "       -8.39896947e-02, -1.38567436e+00, -6.36785865e-01, -2.94503838e-01,\n",
       "       -6.05208538e-02, -1.81972250e-01, -1.02241836e-01,  2.33999386e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3e32842-2862-4d55-a272-e3ae38fd928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vec = sbert_model.encode(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ec9547b-159f-48e4-9622-5b3874970ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3b91373-fff6-439b-88ba-e887456cb363",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 7.230906s; Prediction time: 0.466834s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(sentence_embeddings, Y_train)\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(query_vec)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "# results\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "report = classification_report(Y_test, prediction_linear, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "660e55c7-8e73-49ba-afd2-1aa905e7b01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7839506172839507"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test,prediction_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26575b-8ede-4349-9fc2-ba29ff823bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
